{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 16:51:30.224801: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-09-13 16:52:16.577718: W external/xla/xla/service/gpu/nvptx_compiler.cc:718] The NVIDIA driver's CUDA version is 12.0 which is older than the ptxas CUDA version (12.5.40). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962308b991974389891872e911e393c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing model weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No registered CheckpointArgs found for handler type: <class 'paxml.checkpoints.FlaxCheckpointHandler'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed model weights in 4.44 seconds.\n",
      "Restoring checkpoint from /home/ajshen/.cache/huggingface/hub/models--google--timesfm-1.0-200m/snapshots/8775f7531211ac864b739fe776b0b255c277e2be/checkpoints.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0913 16:52:22.746279    9556 pjrt_stream_executor_client.cc:2809] Execution of replica 0 failed: INVALID_ARGUMENT: executable is built for device CUDA:0 of type \"Tesla V100-PCIE-16GB\"; cannot run it on device CUDA:3 of type \"Tesla P40\"\n",
      "E0913 16:52:22.746264    9554 pjrt_stream_executor_client.cc:2809] Execution of replica 0 failed: INVALID_ARGUMENT: executable is built for device CUDA:0 of type \"Tesla V100-PCIE-16GB\"; cannot run it on device CUDA:2 of type \"Tesla P40\"\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "INVALID_ARGUMENT: executable is built for device CUDA:0 of type \"Tesla V100-PCIE-16GB\"; cannot run it on device CUDA:2 of type \"Tesla P40\": while running replica 0 and partition 2 of a replicated computation (other replicas may have failed as well).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 31\u001b[0m\n\u001b[1;32m     20\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstdout \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m__stdout__\n\u001b[1;32m     22\u001b[0m tfm \u001b[38;5;241m=\u001b[39m timesfm\u001b[38;5;241m.\u001b[39mTimesFm(\n\u001b[1;32m     23\u001b[0m     context_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     24\u001b[0m     horizon_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m720\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     30\u001b[0m )\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtfm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle/timesfm-1.0-200m\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m filenames \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mETT-small/ETTh1.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mETT-small/ETTh2.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124millness/national_illness.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     43\u001b[0m ]\n\u001b[1;32m     45\u001b[0m tvt_ratios \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     46\u001b[0m     (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     47\u001b[0m     (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m     (\u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m),\n\u001b[1;32m     55\u001b[0m ]\n",
      "File \u001b[0;32m~/timeseries-lab/timesfm/src/timesfm.py:270\u001b[0m, in \u001b[0;36mTimesFm.load_from_checkpoint\u001b[0;34m(self, checkpoint_path, repo_id, checkpoint_type, step)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logging(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRestoring checkpoint from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    269\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_state \u001b[38;5;241m=\u001b[39m \u001b[43mcheckpoints\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_state_local_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_specs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_state_partition_specs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logging(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRestored checkpoint in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m )\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# Initialize and jit the decode fn.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tfm_env/lib/python3.10/site-packages/paxml/checkpoints.py:227\u001b[0m, in \u001b[0;36mrestore_checkpoint\u001b[0;34m(state_global_shapes, checkpoint_dir, global_mesh, checkpoint_type, state_specs, step, enforce_restore_shape_check, state_unpadded_shape_dtype_struct, tensorstore_use_ocdbt, restore_transformations)\u001b[0m\n\u001b[1;32m    221\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_dir\u001b[38;5;132;01m=!r}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    222\u001b[0m checkpointer \u001b[38;5;241m=\u001b[39m get_checkpointer(\n\u001b[1;32m    223\u001b[0m     checkpoint_type,\n\u001b[1;32m    224\u001b[0m     enforce_restore_shape_check\u001b[38;5;241m=\u001b[39menforce_restore_shape_check,\n\u001b[1;32m    225\u001b[0m     tensorstore_use_ocdbt\u001b[38;5;241m=\u001b[39mtensorstore_use_ocdbt,\n\u001b[1;32m    226\u001b[0m )\n\u001b[0;32m--> 227\u001b[0m checkpoint_manager \u001b[38;5;241m=\u001b[39m \u001b[43mcheckpoint_managers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOrbaxCheckpointManager\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpointer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensorstore_use_ocdbt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensorstore_use_ocdbt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m   step \u001b[38;5;241m=\u001b[39m checkpoint_manager\u001b[38;5;241m.\u001b[39mlatest_step()\n",
      "File \u001b[0;32m~/miniconda3/envs/tfm_env/lib/python3.10/site-packages/paxml/checkpoint_managers.py:451\u001b[0m, in \u001b[0;36mOrbaxCheckpointManager.__init__\u001b[0;34m(self, directory, checkpointer, train_input_checkpointer, options, checkpoint_type, tensorstore_use_ocdbt, aux_checkpointers)\u001b[0m\n\u001b[1;32m    447\u001b[0m       checkpointers[INPUT_ITEM_NAME] \u001b[38;5;241m=\u001b[39m train_input_checkpointer\n\u001b[1;32m    449\u001b[0m \u001b[38;5;66;03m# Internal Orbax infra configuration\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager \u001b[38;5;241m=\u001b[39m \u001b[43m_CheckpointManagerImpl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpointers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensorstore_use_ocdbt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensorstore_use_ocdbt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tfm_env/lib/python3.10/site-packages/paxml/checkpoint_managers.py:292\u001b[0m, in \u001b[0;36m_CheckpointManagerImpl.__init__\u001b[0;34m(self, directory, checkpointers, options, checkpoint_type, tensorstore_use_ocdbt)\u001b[0m\n\u001b[1;32m    290\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39many_step()\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 292\u001b[0m   version \u001b[38;5;241m=\u001b[39m \u001b[43m_get_checkpoint_version\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_checkpoint_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_digit_step_subdirectory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_use_digit_step_subdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m   logging\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    299\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFound existing checkpoint with version: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, step: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    300\u001b[0m       version,\n\u001b[1;32m    301\u001b[0m       step,\n\u001b[1;32m    302\u001b[0m   )\n\u001b[1;32m    303\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_version:\n",
      "File \u001b[0;32m~/miniconda3/envs/tfm_env/lib/python3.10/site-packages/paxml/checkpoint_managers.py:67\u001b[0m, in \u001b[0;36m_get_checkpoint_version\u001b[0;34m(checkpoint_type, directory, step, use_digit_step_subdirectory)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Necessary because some checkpoints do not conform to Orbax directory\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# structure. Could rely exclusively on actual version if all checkpoints\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# conformed.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint_metadata\u001b[38;5;241m.\u001b[39mmetadata_exists(checkpoint_step_dir):\n\u001b[0;32m---> 67\u001b[0m   version \u001b[38;5;241m=\u001b[39m \u001b[43mcheckpoint_metadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_step_dir\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m     68\u001b[0m       checkpoint_version\u001b[38;5;241m.\u001b[39mget_version_key()\n\u001b[1;32m     69\u001b[0m   ]\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m version\n",
      "File \u001b[0;32m~/miniconda3/envs/tfm_env/lib/python3.10/site-packages/paxml/checkpoint_metadata.py:101\u001b[0m, in \u001b[0;36mrestore_metadata\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     99\u001b[0m checkpointer \u001b[38;5;241m=\u001b[39m Checkpointer(ocp\u001b[38;5;241m.\u001b[39mJsonCheckpointHandler())\n\u001b[1;32m    100\u001b[0m path \u001b[38;5;241m=\u001b[39m directory \u001b[38;5;241m/\u001b[39m METADATA_ITEM_NAME\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcheckpointer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tfm_env/lib/python3.10/site-packages/orbax/checkpoint/checkpointer.py:172\u001b[0m, in \u001b[0;36mCheckpointer.restore\u001b[0;34m(self, directory, *args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m restored \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handler\u001b[38;5;241m.\u001b[39mrestore(directory, args\u001b[38;5;241m=\u001b[39mckpt_args)\n\u001b[1;32m    171\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished restoring checkpoint from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, directory)\n\u001b[0;32m--> 172\u001b[0m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_global_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCheckpointer:restore\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_active_processes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m restored\n",
      "File \u001b[0;32m~/miniconda3/envs/tfm_env/lib/python3.10/site-packages/orbax/checkpoint/multihost/utils.py:140\u001b[0m, in \u001b[0;36msync_global_processes\u001b[0;34m(name, processes)\u001b[0m\n\u001b[1;32m    138\u001b[0m sync_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    139\u001b[0m h \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39muint32(zlib\u001b[38;5;241m.\u001b[39mcrc32(name\u001b[38;5;241m.\u001b[39mencode()))\n\u001b[0;32m--> 140\u001b[0m \u001b[43m_assert_tree_leaves_all_equal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msync_global_processes name mismatch (\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocesses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocesses\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# This may end up just being too noisy given how many barriers there are, but\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# it does represent how long different processes waited around waiting for\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# other processes to reach a barrier.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m jax\u001b[38;5;241m.\u001b[39mmonitoring\u001b[38;5;241m.\u001b[39mrecord_event_duration_secs(\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/jax/checkpoint/sync_global_devices_duration_sec\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    148\u001b[0m     time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m sync_start_time,\n\u001b[1;32m    149\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/tfm_env/lib/python3.10/site-packages/orbax/checkpoint/multihost/utils.py:116\u001b[0m, in \u001b[0;36m_assert_tree_leaves_all_equal\u001b[0;34m(in_tree, fail_message, processes)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_assert_tree_leaves_all_equal\u001b[39m(\n\u001b[1;32m    110\u001b[0m     in_tree: Any,\n\u001b[1;32m    111\u001b[0m     fail_message: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    113\u001b[0m     processes: Optional[Set[\u001b[38;5;28mint\u001b[39m]],\n\u001b[1;32m    114\u001b[0m ):\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Verifies that all the hosts have the same tree of values.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m   expected \u001b[38;5;241m=\u001b[39m \u001b[43mbroadcast_one_to_some\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_tree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocesses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocesses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mtree_util\u001b[38;5;241m.\u001b[39mtree_all(\n\u001b[1;32m    118\u001b[0m       jax\u001b[38;5;241m.\u001b[39mtree_util\u001b[38;5;241m.\u001b[39mtree_map(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39mx: np\u001b[38;5;241m.\u001b[39mall(np\u001b[38;5;241m.\u001b[39mequal(\u001b[38;5;241m*\u001b[39mx)), in_tree, expected)\n\u001b[1;32m    119\u001b[0m   ):\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfail_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Expected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00min_tree\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    122\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/tfm_env/lib/python3.10/site-packages/orbax/checkpoint/multihost/utils.py:91\u001b[0m, in \u001b[0;36mbroadcast_one_to_some\u001b[0;34m(in_tree, is_source, processes)\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(x\u001b[38;5;241m.\u001b[39maddressable_data(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     90\u001b[0m in_tree \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mtree\u001b[38;5;241m.\u001b[39mmap(pre_jit, in_tree)\n\u001b[0;32m---> 91\u001b[0m out_tree \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_psum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msharding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNamedSharding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mglobal_mesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msharding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPartitionSpec\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_tree\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mtree\u001b[38;5;241m.\u001b[39mmap(post_jit, out_tree)\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/tfm_env/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py:1213\u001b[0m, in \u001b[0;36mExecuteReplicated.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1211\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_token_bufs(result_token_bufs, sharded_runtime_token)\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1213\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxla_executable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sharded\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_bufs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mneeds_check_special():\n\u001b[1;32m   1215\u001b[0m   out_arrays \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mdisassemble_into_single_device_arrays()\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: INVALID_ARGUMENT: executable is built for device CUDA:0 of type \"Tesla V100-PCIE-16GB\"; cannot run it on device CUDA:2 of type \"Tesla P40\": while running replica 0 and partition 2 of a replicated computation (other replicas may have failed as well)."
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import external_ts.my_extract as my \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import importlib \n",
    "import timesfm\n",
    "import sys \n",
    "import os\n",
    "importlib.reload(my)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Disable\n",
    "def blockPrint():\n",
    "    sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "# Restore\n",
    "def enablePrint():\n",
    "    sys.stdout = sys.__stdout__\n",
    "\n",
    "tfm = timesfm.TimesFm(\n",
    "    context_len=512,\n",
    "    horizon_len=720,\n",
    "    input_patch_len=32,\n",
    "    output_patch_len=128,\n",
    "    num_layers=20,\n",
    "    model_dims=1280,\n",
    "    backend=\"gpu\",\n",
    ")\n",
    "tfm.load_from_checkpoint(repo_id=\"google/timesfm-1.0-200m\")\n",
    "\n",
    "filenames = [\n",
    "    \"ETT-small/ETTh1.csv\",\n",
    "    \"ETT-small/ETTh2.csv\",\n",
    "    \"ETT-small/ETTm1.csv\",\n",
    "    \"ETT-small/ETTm2.csv\",\n",
    "    \"electricity/electricity.csv\",\n",
    "    \"traffic/traffic.csv\",\n",
    "    \"weather/weather.csv\",\n",
    "    \"exchange_rate/exchange_rate.csv\",\n",
    "    \"illness/national_illness.csv\",\n",
    "]\n",
    "\n",
    "tvt_ratios = [\n",
    "    (1, 0, 0),\n",
    "    (1, 0, 0),\n",
    "    (1, 0, 0),\n",
    "    (1, 0, 0),\n",
    "    (0.7, 0.1, 0.2),\n",
    "    (0.7, 0.1, 0.2),\n",
    "    (0.7, 0.1, 0.2),\n",
    "    (0.7, 0.1, 0.2),\n",
    "    (0.7, 0.1, 0.2),\n",
    "]\n",
    "\n",
    "input_length = 512\n",
    "ili_horizons = [24, 36, 48, 60]\n",
    "other_horizons = [96, 192, 336, 720]\n",
    "\n",
    "\n",
    "result_filename = \"timesfm_results.pkl\"\n",
    "try:\n",
    "    with open(result_filename, \"rb\") as file:\n",
    "        results_dict = pkl.load(file) \n",
    "except FileNotFoundError:\n",
    "    results_dict = dict()\n",
    "\n",
    "for filename, tvt_ratio in zip(filenames, tvt_ratios):\n",
    "    print(filename)\n",
    "    input_length = 96 if \"illness\" in filename else 512\n",
    "\n",
    "    horizons = ili_horizons if \"illness\" in filename else other_horizons\n",
    "    min_horizon = min(horizons)\n",
    "    train_loader, val_loader, test_loader = my.get_timeseries_dataloaders(\n",
    "        f\"./datasets/all_six_datasets/{filename}\", \n",
    "        batch_size=32,\n",
    "        seq_len=input_length,\n",
    "        forecast_horizon=1,\n",
    "        train_ratio=tvt_ratio[0],\n",
    "        val_ratio=tvt_ratio[1],\n",
    "        test_ratio=tvt_ratio[2]\n",
    "    )\n",
    "    train_data, val_data, test_data = train_loader.dataset, val_loader.dataset, test_loader.dataset\n",
    "    train_df, val_df, test_df = pd.DataFrame(train_data.data), pd.DataFrame(val_data.data), pd.DataFrame(test_data.data)\n",
    "\n",
    "    raw_data = pd.read_csv(f\"./datasets/all_six_datasets/{filename}\")\n",
    "    test_df[\"ds\"] = raw_data.iloc[train_data.test_start: train_data.test_end, :][\"date\"].to_numpy()\n",
    "    test_df[\"ds\"] = pd.to_datetime(test_df[\"ds\"])\n",
    "\n",
    "\n",
    "    if (filename, min_horizon) not in results_dict or \"illness\" in filename:\n",
    "        mses = {h: list() for h in horizons}\n",
    "        for t in tqdm(range(len(test_df) - input_length - min_horizon + 1), desc=filename):\n",
    "            test_df_melted = test_df.iloc[t:t+input_length].melt([\"ds\"], var_name=\"unique_id\", value_name=\"values\")\n",
    "\n",
    "            blockPrint()\n",
    "            preds = tfm.forecast_on_df(\n",
    "                test_df_melted, \n",
    "                freq=\"H\",\n",
    "            )\n",
    "            enablePrint()\n",
    "            \n",
    "            for horizon in horizons:\n",
    "                if t + input_length + horizon > len(test_df):\n",
    "                    continue\n",
    "                step_mse = ((\n",
    "                    preds[['unique_id', 'ds', 'timesfm']].pivot(columns=\"unique_id\", values=\"timesfm\", index=\"ds\").iloc[0:horizon, :].to_numpy() - \n",
    "                    test_df.iloc[t+input_length:t+input_length+horizon].drop(columns=[\"ds\"]).to_numpy()\n",
    "                )**2).mean()\n",
    "                mses[horizon].append(step_mse)\n",
    "    \n",
    "        for horizon in horizons:\n",
    "            results_dict[(filename, horizon)] = mses[horizon]\n",
    "\n",
    "\n",
    "        with open(result_filename, \"wb\") as file:\n",
    "            pkl.dump(results_dict, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "with open(\"timesfm_results.pkl\", \"rb\") as file:\n",
    "    results_dict = pkl.load(file)\n",
    "\n",
    "for key, value in results_dict.items():\n",
    "    print(key, np.mean(value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
